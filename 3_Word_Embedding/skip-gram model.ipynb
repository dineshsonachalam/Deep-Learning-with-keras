{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skipgram - > Transforms a sequence of word indexes (list of int) into couples of the form:\n",
    "\n",
    "- (word, word in the same window), with label 1 (positive samples).\n",
    "- (word, random word from the vocabulary), with label 0 (negative samples).\n",
    "\n",
    "\n",
    "\n",
    "**Return: tuple (couples, labels).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import all Necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from keras.preprocessing.text import *\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text to be anlayzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love green eggs and ham .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Declare the tokenizer and run text against it -> produces list of word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'green': 3, 'eggs': 4, 'and': 5, 'ham': 6}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    "\n",
    "# Words are called tokens and the process of splitting text into tokens is called tokenization.\n",
    "\n",
    "# Tokenizer --> creates a dictionary mapping each unique word to an integer ID \n",
    "#               and make it available in word_index attribute.\n",
    "\n",
    "# Dictionary -->(key,value)\n",
    "word2id = tokenizer.word_index\n",
    "print(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'i', 2: 'love', 3: 'green', 4: 'eggs', 5: 'and', 6: 'ham'}\n"
     ]
    }
   ],
   "source": [
    "id2word = {v:k for k,v in word2id.items()}\n",
    "print(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. We convert our input list of words to a list of IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n",
      "['i', 'love', 'green', 'eggs', 'and', 'ham'] \n",
      "\n",
      "i -------ID---> 1\n",
      "love -------ID---> 2\n",
      "green -------ID---> 3\n",
      "eggs -------ID---> 4\n",
      "and -------ID---> 5\n",
      "ham -------ID---> 6\n"
     ]
    }
   ],
   "source": [
    "word_ids = [word2id[w] for w in text_to_word_sequence(text)] # contains word ids for the text. \n",
    "print(word_ids)\n",
    "# Keras provides the text_to_word_sequence() function that you can use to split text into a list of words.\n",
    "\n",
    "print(text_to_word_sequence(text),'\\n')\n",
    "for w in text_to_word_sequence(text):\n",
    "    print(w,'-------ID--->',word2id[w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Then passing it to the skipgrams function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[6, 4], [3, 2], [5, 5], [4, 5], [2, 5], [3, 4], [6, 4], [3, 5], [1, 1], [1, 2], [6, 3], [1, 4], [1, 1], [5, 4], [2, 4], [4, 4], [2, 2], [5, 4], [6, 2], [5, 5], [5, 1], [4, 4], [4, 1], [5, 4], [4, 5], [5, 3], [4, 2], [2, 5], [4, 3], [1, 5], [3, 3], [3, 6], [6, 2], [1, 3], [2, 6], [6, 4], [3, 3], [5, 6], [3, 4], [6, 5], [3, 2], [2, 1], [3, 5], [5, 2], [6, 4], [2, 3], [2, 3], [4, 6], [1, 5], [3, 1], [4, 1], [5, 2], [2, 5], [1, 3], [4, 5], [2, 3]], [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0])\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "PAIRS\n",
      "[[6, 5], [3, 3], [4, 5], [1, 4], [2, 1], [3, 6], [6, 2], [5, 4], [3, 4], [2, 4], [4, 3], [4, 3], [1, 5], [3, 2], [3, 1], [4, 5], [1, 5], [2, 5], [1, 2], [4, 6], [2, 2], [5, 1], [2, 1], [4, 1], [5, 3], [4, 5], [5, 2], [2, 3], [2, 6], [3, 1], [2, 3], [6, 4], [3, 2], [1, 5], [1, 3], [4, 5], [6, 4], [5, 2], [5, 5], [4, 4], [3, 5], [1, 1], [1, 3], [6, 5], [3, 1], [5, 5], [6, 4], [2, 2], [4, 2], [6, 3], [5, 4], [2, 4], [3, 4], [6, 1], [5, 6], [5, 3]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LABELS\n",
      "[0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "pairs, labels = skipgrams(word_ids, len(word2id))\n",
    "print(skipgrams(word_ids, len(word2id))) # word_ids\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"PAIRS\")\n",
    "print(pairs)\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"LABELS\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs Length: 56 \n",
      "Label Length: 56\n"
     ]
    }
   ],
   "source": [
    "print(\"Pairs Length:\",len(pairs),\"\\nLabel Length:\",len(labels)) # We are generating 56 pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "ham\n",
      "5\n",
      "and\n",
      "<class 'list'>\n",
      "Dimension of pairs: (56, 2)\n",
      "I love green eggs and ham .\n"
     ]
    }
   ],
   "source": [
    "print(pairs[0][1])\n",
    "print(id2word[pairs[0][0]])\n",
    "print(pairs[0][1])\n",
    "print(id2word[pairs[0][1]])\n",
    "print(type(pairs))\n",
    "\n",
    "pairs_dimension = np.array(pairs)\n",
    "print(\"Dimension of pairs:\",pairs_dimension.shape)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. We then print the first 10 of the 56 (pair, label) skip-gram tuples generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ham (6), and (5)) -> 0\n",
      "(green (3), green (3)) -> 0\n",
      "(eggs (4), and (5)) -> 0\n",
      "(i (1), eggs (4)) -> 1\n",
      "(love (2), i (1)) -> 1\n",
      "(green (3), ham (6)) -> 1\n",
      "(ham (6), love (2)) -> 1\n",
      "(and (5), eggs (4)) -> 0\n",
      "(green (3), eggs (4)) -> 0\n",
      "(love (2), eggs (4)) -> 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "    id2word[pairs[i][0]], pairs[i][0],\n",
    "    id2word[pairs[i][1]], pairs[i][1],\n",
    "    labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that your results may be different since the skip-gram\n",
    "method randomly samples the results from the pool of possibilities for the positive examples.\n",
    "\n",
    "\n",
    "Additionally, the process of negative sampling, used for generating the negative examples, consists of\n",
    "randomly pairing up arbitrary tokens from the text. As the size of the input text increases, this is more\n",
    "likely to pick up unrelated word pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
